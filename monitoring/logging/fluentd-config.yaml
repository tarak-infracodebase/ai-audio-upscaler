# Fluentd Configuration for Log Aggregation
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: ai-upscaler
data:
  fluent.conf: |
    # Input: Collect logs from Kubernetes containers
    <source>
      @type tail
      @id kubernetes_logs
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      read_from_head true
      format json
      time_format %Y-%m-%dT%H:%M:%S.%NZ
    </source>

    # Input: Collect systemd logs
    <source>
      @type systemd
      @id systemd_logs
      path /run/log/journal
      matches [{ "_SYSTEMD_UNIT": "kubelet.service" }]
      read_from_head true
      tag systemd.kubelet
    </source>

    # Filter: Parse Kubernetes metadata
    <filter kubernetes.**>
      @type kubernetes_metadata
      @id kubernetes_metadata_filter
      kubernetes_url "#{ENV['KUBERNETES_SERVICE_HOST']}:#{ENV['KUBERNETES_SERVICE_PORT_HTTPS']}"
      verify_ssl true
      ca_file /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
      bearer_token_file /var/run/secrets/kubernetes.io/serviceaccount/token
      cache_size 1000
      cache_ttl 3600
      watch true
    </filter>

    # Filter: Exclude non-AI Upscaler logs
    <filter kubernetes.**>
      @type grep
      <regexp>
        key $.kubernetes.namespace_name
        pattern ^ai-upscaler$
      </regexp>
    </filter>

    # Filter: Parse structured logs (JSON)
    <filter kubernetes.**>
      @type parser
      key_name log
      reserve_data true
      reserve_time true
      <parse>
        @type json
        time_key time
        time_format %Y-%m-%dT%H:%M:%S.%fZ
      </parse>
    </filter>

    # Filter: Add environment labels
    <filter kubernetes.**>
      @type record_transformer
      <record>
        environment production
        cluster ai-upscaler-aks
        service_name ${record.dig("kubernetes", "labels", "app") || "unknown"}
        pod_name ${record.dig("kubernetes", "pod_name")}
        namespace ${record.dig("kubernetes", "namespace_name")}
      </record>
    </filter>

    # Filter: Enhance AI Upscaler specific logs
    <filter kubernetes.** ai-audio-upscaler**>
      @type record_transformer
      enable_ruby true
      <record>
        # Add request correlation
        request_id ${record.dig("request_id") || "unknown"}
        user_id ${record.dig("user_id") || "anonymous"}

        # Processing job information
        job_id ${record.dig("job_id")}
        processing_duration ${record.dig("processing_duration")}

        # Error categorization
        error_category ${
          if record.dig("error")
            case record.dig("error").to_s
            when /authentication/i then "auth_error"
            when /authorization/i then "authz_error"
            when /validation/i then "validation_error"
            when /out of memory/i then "oom_error"
            when /timeout/i then "timeout_error"
            when /network/i then "network_error"
            else "unknown_error"
            end
          else
            nil
          end
        }

        # Performance metrics
        response_time_ms ${record.dig("process_time") ? (record["process_time"].to_f * 1000).round(2) : nil}
      </record>
    </filter>

    # Output: Send to Azure Log Analytics
    <match kubernetes.**>
      @type azure_log_analytics
      @id azure_log_analytics_output
      workspace_id "#{ENV['AZURE_LOG_ANALYTICS_WORKSPACE_ID']}"
      shared_key "#{ENV['AZURE_LOG_ANALYTICS_SHARED_KEY']}"
      log_type AIAudioUpscalerLogs
      time_generated_field time
      add_time_field true
      localtime false

      # Buffer configuration for reliability
      <buffer>
        @type file
        path /var/log/fluentd-buffers/azure.buffer
        flush_mode interval
        flush_interval 10s
        flush_thread_count 2
        chunk_limit_size 10MB
        queue_limit_length 512
        retry_type exponential_backoff
        retry_wait 1s
        retry_max_interval 60s
        retry_max_times 5
        overflow_action drop_oldest_chunk
      </buffer>

      # Format for structured logging
      <format>
        @type json
      </format>
    </match>

    # Output: Send errors to separate table
    <match kubernetes.** ai-audio-upscaler**>
      @type rewrite_tag_filter
      <rule>
        key level
        pattern ^(ERROR|CRITICAL|FATAL)$
        tag error.${tag}
      </rule>
    </match>

    <match error.**>
      @type azure_log_analytics
      @id azure_log_analytics_errors
      workspace_id "#{ENV['AZURE_LOG_ANALYTICS_WORKSPACE_ID']}"
      shared_key "#{ENV['AZURE_LOG_ANALYTICS_SHARED_KEY']}"
      log_type AIAudioUpscalerErrors
      time_generated_field time
      add_time_field true
      localtime false

      <buffer>
        @type file
        path /var/log/fluentd-buffers/azure-errors.buffer
        flush_mode interval
        flush_interval 5s
        chunk_limit_size 1MB
        queue_limit_length 256
        retry_max_times 3
      </buffer>
    </match>

    # Output: Debug - stdout for development
    # <match **>
    #   @type stdout
    # </match>

---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: ai-upscaler
spec:
  selector:
    matchLabels:
      name: fluentd
  template:
    metadata:
      labels:
        name: fluentd
    spec:
      serviceAccountName: fluentd
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
      containers:
        - name: fluentd
          image: fluent/fluentd-kubernetes-daemonset:v1-debian-azureloganalytics
          env:
            - name: AZURE_LOG_ANALYTICS_WORKSPACE_ID
              valueFrom:
                secretKeyRef:
                  name: azure-log-analytics
                  key: workspace-id
            - name: AZURE_LOG_ANALYTICS_SHARED_KEY
              valueFrom:
                secretKeyRef:
                  name: azure-log-analytics
                  key: shared-key
            - name: FLUENTD_SYSTEMD_CONF
              value: "disable"
          resources:
            limits:
              memory: 512Mi
              cpu: 200m
            requests:
              memory: 200Mi
              cpu: 100m
          volumeMounts:
            - name: varlog
              mountPath: /var/log
            - name: varlibdockercontainers
              mountPath: /var/lib/docker/containers
              readOnly: true
            - name: config-volume
              mountPath: /fluentd/etc/fluent.conf
              subPath: fluent.conf
            - name: runlogjournal
              mountPath: /run/log/journal
              readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
        - name: varlog
          hostPath:
            path: /var/log
        - name: varlibdockercontainers
          hostPath:
            path: /var/lib/docker/containers
        - name: runlogjournal
          hostPath:
            path: /run/log/journal
        - name: config-volume
          configMap:
            name: fluentd-config

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: fluentd
  namespace: ai-upscaler

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: fluentd
rules:
  - apiGroups: [""]
    resources:
      - pods
      - namespaces
    verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: fluentd
roleRef:
  kind: ClusterRole
  name: fluentd
  apiGroup: rbac.authorization.k8s.io
subjects:
  - kind: ServiceAccount
    name: fluentd
    namespace: ai-upscaler